{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Notebook For Generating Semantic Graphs from Priming Data\n",
        "\n",
        "This notebook contains Python code for generating semantic graphs from the semantic priming data collected by Hutchinson et al. (as it is stored in this repository). To run the code, you will need to set up a Google Cloud Project (GCP) with the VertexAI API enabled."
      ],
      "metadata": {
        "id": "LrlbaPIIo-Ea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Imports"
      ],
      "metadata": {
        "id": "pJSiB93Np8s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "import sys\n",
        "import typing_extensions as typing\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import google.cloud.aiplatform as aiplatform\n",
        "import vertexai\n",
        "from vertexai.generative_models import (\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    Image,\n",
        "    Part,\n",
        "    SafetySetting,\n",
        ")\n",
        "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "import networkx as nx\n",
        "from numba import jit\n",
        "from functools import partial\n"
      ]
    {
      "cell_type": "markdown",
      "source": [
        "###Connect to GCP and Define Functions for Bulk Inference"
      ],
      "metadata": {
        "id": "ZRFpBADwqBPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"YOUR PROJECT ID\"\n",
        "LOCATION = \"YOUR LOCATION\"\n",
        "\n",
        "SERVICE_ACCOUNT = \"YOUR SERVICE ACCOUNT\"\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "mod = \"SET MODEL\"\n",
        "\n",
        "MODEL = GenerativeModel(mod)\n",
        "\n",
        "class Triple(typing.TypedDict):\n",
        "    subject: str\n",
        "    target: str\n",
        "    relation: str\n",
        "\n",
        "def model_generator(prompt: str, temperature: int,\n",
        "                    max_output_tokens: int) -> str:\n",
        "  \"\"\"\n",
        "  Generates an inference from the model.\n",
        "\n",
        "  Args:\n",
        "    prompt: A string prompt.\n",
        "\n",
        "  Returns:\n",
        "    A string response from the model.\n",
        "  \"\"\"\n",
        "  response = MODEL.generate_content(prompt,\n",
        "        generation_config=GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        max_output_tokens=max_output_tokens,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema = {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"subject\": {\"type\": \"string\"},\n",
        "                    \"target\": {\"type\": \"string\"},\n",
        "                    \"relation\": {\"type\": \"string\"}\n",
        "                },\n",
        "                \"required\": [\"subject\", \"target\", \"relation\"]\n",
        "              }\n",
        "          }\n",
        "    ),\n",
        "   safety_settings = [\n",
        "            SafetySetting(\n",
        "              category = HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "              threshold = HarmBlockThreshold.BLOCK_NONE\n",
        "            ),\n",
        "            SafetySetting(\n",
        "                category = HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "              threshold = HarmBlockThreshold.BLOCK_NONE\n",
        "            ),\n",
        "            SafetySetting(\n",
        "              category = HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "              threshold = HarmBlockThreshold.BLOCK_NONE\n",
        "            ),\n",
        "            SafetySetting(\n",
        "              category = HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "              threshold = HarmBlockThreshold.BLOCK_NONE\n",
        "            ),\n",
        "            SafetySetting(\n",
        "              category = HarmCategory.HARM_CATEGORY_UNSPECIFIED,\n",
        "              threshold = HarmBlockThreshold.BLOCK_NONE\n",
        "            ),\n",
        "          ])\n",
        "  try:\n",
        "    return str(response.text)\n",
        "  except ValueError:\n",
        "    return \"BLOCKED\"\n",
        "\n",
        "def batch_sampler(prompts: list[str],temperature: int,\n",
        "                  max_output_tokens:  int) -> list[str]:\n",
        "  \"\"\"Prompts the LLM client with a batch of prompts using ThreadPoolExecutor.\"\"\"\n",
        "  while True:\n",
        "    try:\n",
        "      with ThreadPoolExecutor(max_workers=50) as executor:\n",
        "        func = partial(model_generator, temperature=temperature,\n",
        "                       max_output_tokens=max_output_tokens)\n",
        "        responses = executor.map(func, prompts)\n",
        "      results = [r for r in responses]\n",
        "      break\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "  return results"
      ],
      "metadata": {
        "id": "gTDt-m3h1qW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define the Prompt for Building Semantic Networks"
      ],
      "metadata": {
        "id": "bUChFtBhqHHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KG_PROMPT = \"\"\"\n",
        "You are a top tier algorithm that takes as input a pair consisting of: 1) a\n",
        "subject word, and 2) a list of allowed target words,\n",
        "and returns a list of JSON dictionaries consisting of all relations between the\n",
        "subject and the allowed target words. The list should be [enclosed in brackets].\n",
        "Each JSON dictionary in the outputted list must contain the following keys:\n",
        "  -- 'subject': the subject word.\n",
        "  -- 'target':  the target word that stands in a relation to the subject word.\n",
        "  -- 'relation': a one-to-three word phrase that describes the relationship\n",
        "      between the subject word and the target.\n",
        "All dictionaries in the list must be well-formatted JSON. Not all allowed target\n",
        "words need  to be related to the subject word. The subject and the target should\n",
        "never be the same word. Do not use any targets not in the list of allowed target\n",
        "words.\n",
        "\n",
        "Here is the subject word: {subject_word}\n",
        "Here is the list of allowed target words: {target_words}\n",
        "\n",
        "Output only a list. Your output must begin with \"[\" and end with \"]\".\n",
        "Each dictionary must begin with a bracket and end with a bracket.\n",
        "Each dictionary must be well-formatted JSON with \"subject\", \"target\" and\n",
        "\"relation\" keys. Non-compliance will result in termination.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jO0fSOXN1y11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load Semantic Priming Data"
      ],
      "metadata": {
        "id": "whuGbzQFqTFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data/priming_data.csv', engine='python')\n",
        "words_df = df[df['type'] != 'nw']\n",
        "words_df = words_df[words_df['RT'] != '#NULL!']\n",
        "words_df['RT'] = [int(i) for i in words_df[\"RT\"]]\n",
        "words_df = words_df.reset_index(drop=True)\n",
        "unique_words = np.concatenate([np.unique(words_df['prime']),\n",
        "                               np.unique(words_df['target'])])\n",
        "unique_words = [w.lower() for w in unique_words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0nnZHsi3k-n",
        "outputId": "fd697d18-fffa-421e-f129-0048ed9ebbdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qP8e6g78x9UwV1BJZIr2Jr_FwkoPbRjB\n",
            "To: /content/priming_data.csv\n",
            "100% 32.3M/32.3M [00:00<00:00, 101MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generate the Prompts for All Unique Words"
      ],
      "metadata": {
        "id": "0E5wHeD5qdz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_word(word:str, words:list[str]) -> list[str]:\n",
        "  return [w for w in words if w.lower() != word.lower()]\n",
        "\n",
        "unique_words = [w.lower() for w in unique_words]\n",
        "\n",
        "prompts = [KG_PROMPT.format(subject_word = word,\n",
        "                        target_words = str(remove_word(word,unique_words)))\n",
        "                          for word in unique_words]"
      ],
      "metadata": {
        "id": "IWlL4eoakeqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Set the Temperature and Max Output Token Parameters"
      ],
      "metadata": {
        "id": "Ve3O2m7oqkT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 0\n",
        "max_output_tokens = 512"
      ],
      "metadata": {
        "id": "pvJZ0NLeq2qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Serve Model with all Prompts for all Parameters; Process and Save Output"
      ],
      "metadata": {
        "id": "NVCliQr1rCrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "index = 0\n",
        "knowledge_graph = []\n",
        "while index < len(prompts):\n",
        "  target_prompts = prompts[index : index + BATCH_SIZE]\n",
        "  knowledge_graph.extend(batch_sampler(target_prompts,temperature,\n",
        "                                        max_output_tokens))\n",
        "  message = f'''Temperature: {temperature}. Max Output: {max_output_tokens}.\n",
        "  {index + BATCH_SIZE} of {len(prompts)} prompts processed.\n",
        "  '''\n",
        "  print(message)\n",
        "  index += BATCH_SIZE\n",
        "  time.sleep(30)\n",
        "\n",
        "formatted_graphs = []\n",
        "for graph in tqdm(knowledge_graph):\n",
        "  try:\n",
        "    formatted_graphs.append(ast.literal_eval(graph))\n",
        "  except SyntaxError:\n",
        "    index = graph.rfind('},')\n",
        "    new_graph = graph[0:index] + '}]'\n",
        "    formatted_graphs.append(ast.literal_eval(new_graph))\n",
        "\n",
        "filename = (f'/content/gdrive/My Drive/Text-to-KG and Semantic Priming/'\n",
        "            f'Graph Strings/graph_model_{mod}_temp{temperature}_maxoutput'\n",
        "            f'{max_output_tokens}.txt')\n",
        "with open(filename, 'w') as f:\n",
        "  f.write(str(formatted_graphs))"
      ],
      "metadata": {
        "id": "sG6RItsd2D3b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
